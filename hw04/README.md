## Легенда
Вам хочется найти схожие документы.

## Подробности о задаче

У вас есть некоторое количество страниц википедии, принесённых пауком, например. Страницы -- это просто текстовые файлы. Вы хотите найти похожие страницы. Под похожестью страниц P1 и P2 вы понимаете отношение размера пересечения множеств слов этих страниц к размеру объединения множеств слов. Вы хотите найти 20 самых похожих пар.
 
## Паук

Пауком (aka web crawler) в вашем случае будет простой скрипт на Питоне. Для его запуска необходимы питоновские модули ```mwclient``` и ```mwparserfromhell```. Поставьте их ```pip```ом
или вашим менеджером пакетов. Паук запускается командой ```python crawl-corpus.py``` и по умолчанию скачивает категорию _Distributed file systems_ в каталог _data_. Паук создает для каждой скачанной страницы шард с названием ```pageNNN```, где _NNN_ - порядковый номер страницы и файл ```shards.txt```, в котором перечислены все шарды. В каждом шарде записан текст страницы.

## Инфраструктура Spark

Смотрите инструкции по запуску Spark в каталоге `../class06`

## Запуск решения

Решение оформите так же, как и `wordcount` в каталоге `../class06`: напишите модуль на питоне, который можно будет скормить оболочке pyspark аргументом `--py-files` и запустить командами `import` и `run`. В файле `hw04.pyspark` Напишите пару строк, которые необходимо выполнить в оболочке (чтоб проверяющим не гадать, что же тут запускать)


## Ограничения и пожелания

Равномерная загрузка узлов кластера и эффективное вычисление
