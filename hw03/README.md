## Легенда
Вам нужно сделать простой ранжированный булевский поиск по страницам Википедии, в котором
запросом является конъюнкция слов. В документах, попавших в результат, должны присутствовать все слова запроса.

## Подробности о задаче

У вас есть некоторое количество страниц википедии, принесённых пауком, например. У страниц есть небольшая структура: название, список авторов и тело. Вам нужно написать конвейер задач map-reduce, строящий инвертированный индекс, и реализацию поисковика, использующего полученный инвертированный индекс для булевского поиска с ранжированием.


## Паук

Пауком (aka web crawler) в вашем случае будет простой скрипт на Питоне. Для его запуска необходимы питоновские модули ```mwclient``` и ```mwparserfromhell```. Поставьте их ```pip```ом
или вашим менеджером пакетов. Паук запускается командой ```python crawl-corpus.py``` и по умолчанию скачивает категорию _Distributed file systems_ в каталог _data_. Паук создает для каждой скачанной страницы шард с названием ```pageNNN```, где _NNN_ - порядковый номер страницы и файл ```shards.txt```, в котором перечислены все шарды. В каждом шарде записан JSON объект с
полями "ID" (номер страницы, равен упомянутому выше значению _NNN_), "Authors" (массив имен авторов страницы), "Title" (название страницы) и "Body" (текст страницы)

## Инфраструктура Map-Reduce

В каталогах `../mapreduce/{lin,win,mac}` находится бинарник `mapreduce`, скомпилированный для
каждой из этих трёх платформ. Этот бинарник умеет использовать в качестве реализаций
функций map и reduce скрипты на питоне. Инструкции по запуску процесса и пример wordcount находятся в каталоге `../mapreduce`

Обратите внимание, что если вы подаете выход одного звена конвейера на вход другому, то для корректной работы инфраструктуры вам понадобится сгенерировать новый файл-каталог. содержащий в себе пути к новым шардам.

## Заготовка
В файлах ```index_mapper.py``` и ```index_reducer.py``` реализована заготовка, строящая примитивный инвертированный индекс. Вы можете использовать её в работе.

## Запуск решения
Напишите демонстрационный скрипт на bash или python, который запустит построение индекса поверх данного ему корпуса, после чего выполнит переданный запрос, назначив
зонам документов заданные веса. Запуск скрипта должен выглядеть так:

```
[интерпретатор] ./search --data data --query "Shard distributed" --weights 0.1,0.5,0.4
```

где

- ```./search``` -- название скрипта, опционально с указанием интерпретатора
- ```--data``` -- каталог с корпусом
- ```--query``` -- запрос в виде термов, разделённых пробелом
- ```--weights``` -- разделённые запятой веса. Первым идёт вес авторов, вторым вес названия, третьим вес тела страницы

Скрипт должен вернуть номера и ранги страниц, удовлетворяющих запросу, в порядке ранжирования (лучшая страница должна идти первой /_примечание Кэпа_/), по одной странице на строке. Пример результата:

    10 5.66
    3 2.39
    7 0.30

В первом столбце записаны номера страниц, во втором полученные ранги. Количество пробелов между столбцами не имеет значения.


## Ограничения и пожелания

Один шард поместится в память одного процесса. Один список вхождений тоже поместится. Все данные в память одного процесса не поместятся ни при индексировании, ни при поиске. Постарайтесь реализовать индексирующий конвейер и поисковик так, чтобы узких мест не было.
